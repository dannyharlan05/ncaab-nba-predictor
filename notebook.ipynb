{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T02:41:30.903319Z",
     "start_time": "2025-05-25T02:41:30.120148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 1 ‚Äî robust fetch of the 360+ team slugs\n",
    "\n",
    "# ‚îÄ‚îÄ 1) install & import cache tools ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "!pip install requests-cache   # run once; comment out after it‚Äôs installed\n",
    "\n",
    "import requests_cache, requests, certifi, re, time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# set up a cached session (caches to 'teamlist_cache.sqlite')\n",
    "requests_cache.install_cache(\"teamlist_cache\", expire_after=86400)\n",
    "session = requests_cache.CachedSession()\n",
    "\n",
    "SEASON   = 2025\n",
    "BASE_URL = f\"https://www.sports-reference.com/cbb/seasons/{SEASON}-school-stats.html\"\n",
    "HEADERS  = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\"}\n",
    "\n",
    "# ‚îÄ‚îÄ 2) fetch once, with retry/back-off ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def fetch_teamlist(url, max_retries=3):\n",
    "    backoff = 1\n",
    "    for attempt in range(1, max_retries+1):\n",
    "        resp = session.get(url, headers=HEADERS,\n",
    "                           verify=certifi.where(), timeout=10)\n",
    "        if resp.status_code == 200:\n",
    "            return resp.text\n",
    "        if resp.from_cache:\n",
    "            # if we‚Äôre reading from cache but it‚Äôs stale, just use it\n",
    "            return resp.text\n",
    "        if resp.status_code == 429:\n",
    "            print(f\"‚ö†Ô∏è  429 rate limit, sleeping {backoff}s (attempt {attempt})\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "        else:\n",
    "            resp.raise_for_status()\n",
    "    raise RuntimeError(f\"Failed to fetch {url} after {max_retries} retries\")\n",
    "\n",
    "html = fetch_teamlist(BASE_URL)\n",
    "\n",
    "# ‚îÄ‚îÄ 3) parse out the team slugs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "soup  = BeautifulSoup(html, \"lxml\")\n",
    "links = soup.select(\"table#basic_school_stats a[href*='/cbb/schools/']\")\n",
    "teams = {\n",
    "    a.text.strip(): re.search(r\"/cbb/schools/([^/]+)/\", a[\"href\"]).group(1)\n",
    "    for a in links\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ  Teams found: {len(teams)}\")   # should be ~360+\n",
    "\n"
   ],
   "id": "669bab2f27c74c88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests-cache in ./.venv/lib/python3.13/site-packages (1.2.1)\r\n",
      "Requirement already satisfied: attrs>=21.2 in ./.venv/lib/python3.13/site-packages (from requests-cache) (25.3.0)\r\n",
      "Requirement already satisfied: cattrs>=22.2 in ./.venv/lib/python3.13/site-packages (from requests-cache) (24.1.3)\r\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./.venv/lib/python3.13/site-packages (from requests-cache) (4.3.8)\r\n",
      "Requirement already satisfied: requests>=2.22 in ./.venv/lib/python3.13/site-packages (from requests-cache) (2.32.3)\r\n",
      "Requirement already satisfied: url-normalize>=1.4 in ./.venv/lib/python3.13/site-packages (from requests-cache) (2.2.1)\r\n",
      "Requirement already satisfied: urllib3>=1.25.5 in ./.venv/lib/python3.13/site-packages (from requests-cache) (2.4.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests>=2.22->requests-cache) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests>=2.22->requests-cache) (3.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests>=2.22->requests-cache) (2025.4.26)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "‚úÖ  Teams found: 364\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T02:41:32.686829Z",
     "start_time": "2025-05-25T02:41:32.402430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 2 ‚Äî fetch_schedule() reads each team‚Äôs table and extracts site via data-stat\n",
    "\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import certifi\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_schedule(slug, team_name):\n",
    "    \"\"\"\n",
    "    Return DataFrame with columns:\n",
    "      date, opponent, team_pts, opp_pts, site, team\n",
    "    \"\"\"\n",
    "    url  = f\"https://www.sports-reference.com/cbb/schools/{slug}/{SEASON}-schedule.html\"\n",
    "    resp = session.get(url, headers=HEADERS, verify=certifi.where(), timeout=10)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    # 1) Load into pandas to get core table\n",
    "    df = pd.read_html(StringIO(resp.text), match=\"Date\", flavor=\"lxml\")[0]\n",
    "    if \"G\" in df.columns:\n",
    "        df = df[df[\"G\"] != \"G\"]\n",
    "\n",
    "    # 2) Scrape the raw <td data-stat=\"game_location\"> cells\n",
    "    soup  = BeautifulSoup(resp.text, \"lxml\")\n",
    "    table = soup.find(\"table\", id=\"schedule\")\n",
    "    sites_raw = [td.get_text(strip=True) for td in table.select(\"td[data-stat='game_location']\")]\n",
    "    site = pd.Series(sites_raw, name=\"site\").map({\"@\":\"away\",\"N\":\"neutral\",\"\": \"home\"}).fillna(\"home\")\n",
    "\n",
    "    # 3) Clean opponent names (strip leading \"@\"/\"N\")\n",
    "    df[\"opponent\"] = df[\"Opponent\"].astype(str).str.lstrip(\"@N \").str.strip()\n",
    "\n",
    "    # 4) Rename date & score columns\n",
    "    col_map = {\"Date\":\"date\", \"Tm\":\"team_pts\", \"Opp\":\"opp_pts\", \"Opp.1\":\"opp_pts\"}\n",
    "    df = df.rename(columns=col_map)\n",
    "\n",
    "    # 5) Assemble final DataFrame\n",
    "    out = pd.DataFrame({\n",
    "        \"date\":     df[\"date\"],\n",
    "        \"opponent\": df[\"opponent\"],\n",
    "        \"team_pts\": df[\"team_pts\"],\n",
    "        \"opp_pts\":  df[\"opp_pts\"],\n",
    "        \"site\":     site.values,\n",
    "        \"team\":     team_name\n",
    "    })\n",
    "    return out\n"
   ],
   "id": "e3510dd81c12b531",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T03:46:54.138021Z",
     "start_time": "2025-05-25T02:41:35.850227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 3 ‚Äî very-safe full-season scrape in batches to avoid rate-limits\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "OUT = f\"ncaa_games_{SEASON}.csv\"\n",
    "\n",
    "if os.path.exists(OUT):\n",
    "    # If the full CSV already exists, just load it\n",
    "    games = pd.read_csv(OUT, parse_dates=[\"date\"])\n",
    "    print(f\"üì• Loaded existing {OUT} ({len(games)} games)\")\n",
    "else:\n",
    "    all_games = []\n",
    "    teams_list = list(teams.items())\n",
    "    batch_size = 20       # number of teams per batch\n",
    "    post_batch_sleep = 300  # 5 minutes between batches\n",
    "\n",
    "    # Loop in batches\n",
    "    for batch_start in range(0, len(teams_list), batch_size):\n",
    "        batch = teams_list[batch_start:batch_start + batch_size]\n",
    "        for team, slug in batch:\n",
    "            try:\n",
    "                sched = fetch_schedule(slug, team)  # from Cell 2; uses cached session\n",
    "                all_games.append(sched)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Skipping {team}: {e}\")\n",
    "            # polite pause 15‚Äì30s between individual requests\n",
    "            delay = random.uniform(15, 30)\n",
    "            print(f\"‚è± Sleeping {delay:.0f}s after {team}\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "        # after each batch, rest for 5 minutes (unless it‚Äôs the last batch)\n",
    "        if batch_start + batch_size < len(teams_list):\n",
    "            print(f\"‚úÖ Batch {batch_start//batch_size + 1} complete; sleeping {post_batch_sleep//60}min\")\n",
    "            time.sleep(post_batch_sleep)\n",
    "\n",
    "    # Combine all team DataFrames\n",
    "    combined = pd.concat(all_games, ignore_index=True)\n",
    "\n",
    "    # De-duplicate so each head-to-head game appears once\n",
    "    combined[\"key\"] = (\n",
    "        combined[\"date\"].astype(str) + \"_\" +\n",
    "        combined[[\"team\",\"opponent\"]]\n",
    "                .astype(str)\n",
    "                .apply(lambda x: \"_\".join(sorted(x)), axis=1)\n",
    "    )\n",
    "    games = (\n",
    "        combined\n",
    "        .sort_values([\"date\",\"team\"])\n",
    "        .drop_duplicates(\"key\")\n",
    "        .drop(columns=\"key\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Compute margin and save\n",
    "    games[\"margin\"] = games[\"team_pts\"] - games[\"opp_pts\"]\n",
    "    games.to_csv(OUT, index=False)\n",
    "    print(f\"‚úÖ Scraped & saved {len(games)} games ‚Üí {OUT}\")\n",
    "\n",
    "# Sanity check\n",
    "display(games.head())\n",
    "print(\"Site counts:\\n\", games.site.value_counts())\n",
    "\n"
   ],
   "id": "f90680a59596f024",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è± Sleeping 19s after Abilene Christian\n",
      "‚è± Sleeping 16s after Air Force\n",
      "‚è± Sleeping 18s after Akron\n",
      "‚è± Sleeping 22s after Alabama\n",
      "‚è± Sleeping 21s after Alabama A&M\n",
      "‚è± Sleeping 22s after Alabama State\n",
      "‚è± Sleeping 20s after Albany (NY)\n",
      "‚è± Sleeping 29s after Alcorn State\n",
      "‚è± Sleeping 21s after American\n",
      "‚è± Sleeping 27s after Appalachian State\n",
      "‚è± Sleeping 23s after Arizona\n",
      "‚è± Sleeping 28s after Arizona State\n",
      "‚è± Sleeping 29s after Arkansas\n",
      "‚è± Sleeping 25s after Arkansas State\n",
      "‚è± Sleeping 20s after Arkansas-Pine Bluff\n",
      "‚è± Sleeping 17s after Army\n",
      "‚è± Sleeping 29s after Auburn\n",
      "‚è± Sleeping 28s after Austin Peay\n",
      "‚è± Sleeping 26s after Ball State\n",
      "‚è± Sleeping 17s after Baylor\n",
      "‚úÖ Batch 1 complete; sleeping 5min\n",
      "‚è± Sleeping 25s after Bellarmine\n",
      "‚è± Sleeping 23s after Belmont\n",
      "‚è± Sleeping 23s after Bethune-Cookman\n",
      "‚è± Sleeping 22s after Binghamton\n",
      "‚è± Sleeping 16s after Boise State\n",
      "‚è± Sleeping 18s after Boston College\n",
      "‚è± Sleeping 20s after Boston University\n",
      "‚è± Sleeping 18s after Bowling Green State\n",
      "‚è± Sleeping 27s after Bradley\n",
      "‚è± Sleeping 29s after Brigham Young\n",
      "‚è± Sleeping 27s after Brown\n",
      "‚è± Sleeping 20s after Bryant\n",
      "‚è± Sleeping 20s after Bucknell\n",
      "‚è± Sleeping 25s after Buffalo\n",
      "‚è± Sleeping 24s after Butler\n",
      "‚è± Sleeping 16s after Cal Poly\n",
      "‚è± Sleeping 25s after Cal State Bakersfield\n",
      "‚è± Sleeping 16s after Cal State Fullerton\n",
      "‚è± Sleeping 16s after Cal State Northridge\n",
      "‚è± Sleeping 26s after California\n",
      "‚úÖ Batch 2 complete; sleeping 5min\n",
      "‚è± Sleeping 29s after California Baptist\n",
      "‚è± Sleeping 22s after Campbell\n",
      "‚è± Sleeping 28s after Canisius\n",
      "‚è± Sleeping 18s after Central Arkansas\n",
      "‚è± Sleeping 29s after Central Connecticut State\n",
      "‚è± Sleeping 24s after Central Michigan\n",
      "‚è± Sleeping 16s after Charleston Southern\n",
      "‚è± Sleeping 26s after Charlotte\n",
      "‚è± Sleeping 26s after Chattanooga\n",
      "‚è± Sleeping 27s after Chicago State\n",
      "‚è± Sleeping 16s after Cincinnati\n",
      "‚è± Sleeping 20s after Clemson\n",
      "‚è± Sleeping 18s after Cleveland State\n",
      "‚è± Sleeping 16s after Coastal Carolina\n",
      "‚è± Sleeping 19s after Colgate\n",
      "‚è± Sleeping 28s after College of Charleston\n",
      "‚è± Sleeping 27s after Colorado\n",
      "‚è± Sleeping 28s after Colorado State\n",
      "‚è± Sleeping 25s after Columbia\n",
      "‚è± Sleeping 27s after Connecticut\n",
      "‚úÖ Batch 3 complete; sleeping 5min\n",
      "‚è± Sleeping 25s after Coppin State\n",
      "‚è± Sleeping 19s after Cornell\n",
      "‚è± Sleeping 19s after Creighton\n",
      "‚è± Sleeping 30s after Dartmouth\n",
      "‚è± Sleeping 28s after Davidson\n",
      "‚è± Sleeping 21s after Dayton\n",
      "‚è± Sleeping 30s after Delaware\n",
      "‚è± Sleeping 28s after Delaware State\n",
      "‚è± Sleeping 17s after Denver\n",
      "‚è± Sleeping 20s after DePaul\n",
      "‚è± Sleeping 17s after Detroit Mercy\n",
      "‚è± Sleeping 17s after Drake\n",
      "‚è± Sleeping 26s after Drexel\n",
      "‚è± Sleeping 22s after Duke\n",
      "‚è± Sleeping 23s after Duquesne\n",
      "‚è± Sleeping 29s after East Carolina\n",
      "‚è± Sleeping 27s after East Tennessee State\n",
      "‚è± Sleeping 18s after East Texas A&M\n",
      "‚è± Sleeping 22s after Eastern Illinois\n",
      "‚è± Sleeping 25s after Eastern Kentucky\n",
      "‚úÖ Batch 4 complete; sleeping 5min\n",
      "‚è± Sleeping 22s after Eastern Michigan\n",
      "‚è± Sleeping 21s after Eastern Washington\n",
      "‚è± Sleeping 20s after Elon\n",
      "‚è± Sleeping 28s after Evansville\n",
      "‚è± Sleeping 28s after Fairfield\n",
      "‚è± Sleeping 23s after FDU\n",
      "‚è± Sleeping 18s after Florida\n",
      "‚è± Sleeping 17s after Florida A&M\n",
      "‚è± Sleeping 30s after Florida Atlantic\n",
      "‚è± Sleeping 16s after Florida Gulf Coast\n",
      "‚è± Sleeping 18s after Florida International\n",
      "‚è± Sleeping 23s after Florida State\n",
      "‚è± Sleeping 18s after Fordham\n",
      "‚è± Sleeping 24s after Fresno State\n",
      "‚è± Sleeping 24s after Furman\n",
      "‚è± Sleeping 29s after Gardner-Webb\n",
      "‚è± Sleeping 27s after George Mason\n",
      "‚è± Sleeping 20s after George Washington\n",
      "‚è± Sleeping 26s after Georgetown\n",
      "‚è± Sleeping 17s after Georgia\n",
      "‚úÖ Batch 5 complete; sleeping 5min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 38\u001B[39m\n\u001B[32m     36\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m batch_start + batch_size < \u001B[38;5;28mlen\u001B[39m(teams_list):\n\u001B[32m     37\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m‚úÖ Batch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbatch_start//batch_size\u001B[38;5;250m \u001B[39m+\u001B[38;5;250m \u001B[39m\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m complete; sleeping \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpost_batch_sleep//\u001B[32m60\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33mmin\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m38\u001B[39m         \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpost_batch_sleep\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     40\u001B[39m \u001B[38;5;66;03m# Combine all team DataFrames\u001B[39;00m\n\u001B[32m     41\u001B[39m combined = pd.concat(all_games, ignore_index=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T00:56:17.670885Z",
     "start_time": "2025-05-25T00:56:16.820695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 3 ‚Äî collect every team‚Äôs DataFrame into a list\n",
    "\n",
    "all_games = []\n",
    "for team, slug in teams.items():\n",
    "    sched = fetch_schedule(slug, team)   # uses your Cell 2 function\n",
    "    all_games.append(sched)\n",
    "\n",
    "# now all_games has 364 DataFrames\n"
   ],
   "id": "95c0bc67426fa17",
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://www.sports-reference.com/cbb/schools/bethune-cookman/2025-schedule.html",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m all_games = []\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m team, slug \u001B[38;5;129;01min\u001B[39;00m teams.items():\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m     sched = \u001B[43mfetch_schedule\u001B[49m\u001B[43m(\u001B[49m\u001B[43mslug\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mteam\u001B[49m\u001B[43m)\u001B[49m   \u001B[38;5;66;03m# uses your Cell 2 function\u001B[39;00m\n\u001B[32m      6\u001B[39m     all_games.append(sched)\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# now all_games has 364 DataFrames\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 20\u001B[39m, in \u001B[36mfetch_schedule\u001B[39m\u001B[34m(slug, team_name)\u001B[39m\n\u001B[32m     13\u001B[39m url  = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mhttps://www.sports-reference.com/cbb/schools/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mslug\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSEASON\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m-schedule.html\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     14\u001B[39m resp = requests.get(\n\u001B[32m     15\u001B[39m     url,\n\u001B[32m     16\u001B[39m     headers=HEADERS,\n\u001B[32m     17\u001B[39m     verify=certifi.where(),\n\u001B[32m     18\u001B[39m     timeout=\u001B[32m10\u001B[39m\n\u001B[32m     19\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m \u001B[43mresp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# 1) Load into pandas to get scores and raw Opponent column\u001B[39;00m\n\u001B[32m     23\u001B[39m df = pd.read_html(StringIO(resp.text), match=\u001B[33m\"\u001B[39m\u001B[33mDate\u001B[39m\u001B[33m\"\u001B[39m, flavor=\u001B[33m\"\u001B[39m\u001B[33mlxml\u001B[39m\u001B[33m\"\u001B[39m)[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.13/site-packages/requests/models.py:1024\u001B[39m, in \u001B[36mResponse.raise_for_status\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1019\u001B[39m     http_error_msg = (\n\u001B[32m   1020\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.status_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m Server Error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreason\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m for url: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.url\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m   1021\u001B[39m     )\n\u001B[32m   1023\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[32m-> \u001B[39m\u001B[32m1024\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response=\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[31mHTTPError\u001B[39m: 429 Client Error: Too Many Requests for url: https://www.sports-reference.com/cbb/schools/bethune-cookman/2025-schedule.html"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T00:57:42.726175Z",
     "start_time": "2025-05-25T00:57:42.720873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Quick check: is 'teams' in your namespace?\n",
    "print(\"teams defined?\", 'teams' in globals())\n",
    "if 'teams' in globals():\n",
    "    print(\"Number of teams:\", len(teams))\n",
    "    print(\"Sample keys:\", list(teams.keys())[:5])\n",
    "\n"
   ],
   "id": "73cffd7390157153",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teams defined? True\n",
      "Number of teams: 364\n",
      "Sample keys: ['Abilene Christian', 'Air Force', 'Akron', 'Alabama', 'Alabama A&M']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d84916737c8168d6"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
